---
title: "Random_walk"
author: "Tempest McCabe"
date: "7/2/2019"
output: html_document
---

```{r}
library(rjags)
library(coda)
library(ggplot2)
library(dplyr)
```

## Developing priors

# Prior on probaility of infection per-person per year


```{r}

# load data
points_of_less_than_six_big <- which(tick_disease_big$Number == "<6")
points_of_less_than_six <- which(tick_disease_big$Number == "<6")


## use non cumberland, non less than six, non 2001-2008 data
tick_disease_big_tmp <- tick_disease_big[tick_disease_big$Number != "<6",]
tick_disease_big_tmp <- tick_disease_big_tmp[!(tick_disease_big_tmp$Location %in% towns_in_cumberland),]
tick_disease_other <- dplyr::filter(tick_disease_big_tmp, tick_disease_big_tmp$Year > 2008)

# estimate the probability that results in a disease status with other population data
get_tick_disease_probability <- "model{
## process model
  for (i in 1:n){
   number_of_disease_risk[i] ~ dbinom(probability, pop[i])
  }

## Prior
probability ~ dbeta(1,1) # close to uniform probability
}
"

data <- list(n = length(tick_disease_other$Number),  number_of_disease_risk = as.numeric(tick_disease_other$Number), pop = as.numeric(tick_disease_other$Population))
  
nchain = 3
init <- list()
for(i in 1:nchain){
  init[[i]] <- list(probability = c(0.15))
}


j.model   <- jags.model (file = textConnection(get_tick_disease_probability),
                             data = data,
                             inits = init,
                             n.chains = 3)

jags.out   <- coda.samples (model = j.model,
                            variable.names = c("probability"),
                                n.iter = 11000)
plot(jags.out)

GBR <- gelman.plot(jags.out)

#burnin <- GBR$last.iter[tail(which(apply(GBR$shrink[,,2]>1.05,1,any)),1)+1] #Statement not working. 
jags.burn <- window(jags.out,start=4000, extend = FALSE)

effectiveSize(jags.burn) # Solid 3x as many as we need
jags.burn <- as.matrix(jags.burn)
plot(density(jags.burn), main = "Probability of Tick disease per population member per year")


### Test: Does this meet the same assumptions of the class? (as in, will we see numbers bounded by  1-6?)
less_than_six_pop <- as.numeric(tick_disease_big$Population[points_of_less_than_six_big])
prob <- sample(jags.burn, size = length(less_than_six_pop))
number_of_tick_disease_incidences <- rbinom(length(less_than_six_pop), less_than_six_pop, prob)

```


```{r}
### Solve for a and b parameters of beta distribution
estBetaParams <- function(mu, var) {
  alpha <- ((1 - mu) / var - 1 / mu) * mu ^ 2
  beta <- alpha * (1 / mu - 1)
  return(params = list(alpha = alpha, beta = beta))
}

beta_params <- estBetaParams(mean_prob, var_prob)

```

Note: Solving the Beta equation parameters useing simultanious equations was actually somewhat non-trivial, and someone already did it. The answer I am useing in from [here](https://stats.stackexchange.com/questions/12232/calculating-the-parameters-of-a-beta-distribution-using-the-mean-and-variance). 

# Prior on error levels of underreporting (tau_obs)

This is a tricky number to get, specifically because it is hard to constrain, and also so useful. We wanted to have a CDC-provided estimate of the error to put into the prior, but also anouther estimate to benchmark our modeled output against. Ideally, the prior would be set by a more general estimate of the error, and we could then benchmark against Maine-level estimates. 

*National Estimates of Errors*

Some national error estimates include [this paper](https://academic.oup.com/cid/article/59/5/676/2895755) by Hinckley et al, which estimates the "true" number of infections in 2008 based on the number of specimines sent into labs, and correcting for false positives and false negatives. It does not estimate the number of incidences where the tick itself was not sent in for testing, but says this could add to the number. 

[This CDC summary](https://www.cdc.gov/mmwr/volumes/66/ss/ss6622a1.htm?s_cid=ss6622a1_w) of the 2008-2015 data notes that tick disease rates are underreported in high-incidence areas but possibly over-reported in low-incidence areas.
  Starting in 2008, the CDC explicitly notes the difference between confirmed cases and probable cases. This difference could be used to estimate some of the error, but is also likely an underestimate because the definition of "probable case" still involves some positive lab results/ medical diagnosis. 

[This estimate](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4550147/) of 2005-2010 national incidence rate useing health insurence claims documents roughly agrees with rate of CDC confirmed cases. 

[This review](https://www-sciencedirect-com.ezproxy.bu.edu/science/article/pii/S0891552015000240?via%3Dihub) as well as being a thourough general review of lyme in the united states, summerizes some of the liturature attemting to estimate the amount of underreporting in lyme disease, putting it aproximatly 3-12 fold differences. The studies were state/ town specific but the techniqes used were: 
 - In Westchestesr New York a "determinisitc model with plausible ranges" [was used](https://academic.oup.com/aje/article/148/10/1018/135572) to estimate "true" incidence levels. 
 - Connecticut Physicians [were surveyed](https://europepmc.org/abstract/med/10186700).
 - Maryland Physicians [were surveyed](https://doi.org/10.1093/infdis/173.5.1260)
 -  Medical records in Wisconsin [were reviewd](https://academic.oup.com/aje/article/155/12/1120/123221)
 
*Maine Estimates of errors*

[There are 2011-2016 estimates of prevelance](https://data.mainepublichealth.gov/tracking/metadata-lyme-prevalence) by town in Maine. Because Prevelance = Incidence x Average duration of disease, we may be able to compare our estimates to prevelance data as measured by the survey. However, prevelance is estimated by the question "Have you ever been told by a doctor, nurse, or other health professional that you have Lyme disease?”. This question does not capture populations that would not have intereacted with medical profesionals. 


```{r}

### Fit random walk

RandomWalk = "
model{
  
  #### Data Model
  for(t in numeric_indices){
    y[t] ~ dnorm(x[t],tau_obs)
  }
  
  for(t in less_than_six_indices){
   y[t] ~ dbinom(probability, pop[t]) T(1,6) ## Include suppresed data
   y[t] ~ dnorm(x[t], tau_obs)
  }

  #### Process Model
  for(t in 2:n){
    x[t]~dnorm(x[t-1],tau_add)
  }
  
  #### Priors
  x[1] ~ dnorm(x_ic,tau_ic) # Could get from: non-cumberland data, 2007 data, canadian data
  tau_obs ~ dgamma(a_obs,r_obs) # not sure where, need multiple error estimates
  tau_add ~ dgamma(a_add,r_add) # could estimate from: non-cumberland data, years we aren't useing, nearby data. 
  probability ~ dbeta(beta_params[1], beta_params[2])
}
"



data <- list(points_of_less_than_six = , beta_params = beta_params, 
            )

nchain = 3
init <- list()
for(i in 1:nchain){
  init[[i]] <- list(m = 0.5, D_0 = 0.016, min_conductance = 0.001)
}


j.model   <- jags.model (file = textConnection(Stomtal_slope),
                             data = data,
                             inits = init,
                             n.chains = 3)

jags.out   <- coda.samples (model = j.model,
                            variable.names = c("m","min_conductance","D_0", "precision"),
                                n.iter = n.iter)
GBR <- gelman.plot(jags.out)
burnin <- GBR$last.iter[tail(which(apply(GBR$shrink[,,2]>1.05,1,any)),1)+1]
if(is.na(burnin)){
  warning("GBR !< 1.05. Model may have failed to converge")
  jags.burn <- jags.out
  did_it_converge <- "convergence_failed_GBR_test" 
}else{
  did_it_converge <- "convergence_passed_GBR_test"
  jags.burn <- window(jags.out,start=burnin, extend = FALSE)
}
  date_stamp <- Sys.time()
  date_stamp <- format(date_stamp, "%Y%m%d")
  file_name <- paste("/Users/tess/Documents/work/SERDP_Project/Cogongrass_trait_sampling/Jags_output/", date_stamp, ".", "Stomatal_slope_of",".",leaf_name,".", did_it_converge, ".","JAGS_run.Rdata", sep = "")
  print(did_it_converge)
  print(effectiveSize(jags.burn))
  save(jags.burn, file = file_name )
  #return(jags.burn)
}

### Testing function
#temp_data <- dplyr::filter(full_curve, Name == "tess-feild-3")
#run_stomatal_slop_fit(temp_data = temp_data, leaf_name = "tess-feild-3", n.iter = 6000 )
#effectiveSize(jags.burn)

## Loop over each file (corresponding to each leaf)
leaves <- unique(full_curve$Name)
leaves <- leaves[!is.na(leaves)]

for ( i in seq_along(leaves)){
  temp_data <- dplyr::filter(full_curve, Name == leaves[i])
  run_stomatal_slop_fit(temp_data = temp_data, leaf_name = leaves[i], n.iter = 6000 )
 
}

```
